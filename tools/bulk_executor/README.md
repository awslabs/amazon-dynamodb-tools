# Bulk Executor for Amazon DynamoDB

Bulk Executor for Amazon DynamoDB lets you efficiently run bulk commands against even large tables. It:

* Appears like a command-line utility
* Calls an AWS Glue Spark job in the background for massive parallel execution
* Provides a variety of useful bulk actions (like finding items matching some advanced criteria and printing them, counting them, updating, or deleting them)
* Has an extension mechanism for adding new bulk actions via simple script placement

## Usage samples

Here are some example use cases:

```
# Count the number of items in a table
# Internally uses the DynanmoDB connector to read items using a background scan
./bulk count --table t

# Count the items matching some arbitrary criteria
# The criteria supports the Spark SQL syntax
# https://spark.apache.org/docs/latest/api/sql/index.html
# This sample finds items where the a attribute is larger than the b attribute, 
# the ts is before 2024, and val is from the given set.
./bulk count --table t --where "a > b and ts < '2024' and val = ('x' or 'y')"


# It's possible to print the results instead of counting
# The first 10 items appear on the command line, one item per line
# Large result sets are written to S3
./bulk find --table t --where "a > b and ts < '2024' and val = ('x' or 'y')"

# Print the 100 oldest items, using the Connector, no GSI required
./bulk find --table t --orderby timestamp --limit 100

# Print the 100 newest items
./bulk find --table t --orderby timestamp desc --limit 100


# Delete the 100 oldest items
./bulk delete --table t --orderby timestamp --limit 100

# Delete all items with a timestamp before some threshold
./bulk delete --table t --where "timestamp < '2024-01-01'"


# Insert 100 million fake items (for testing)
# Items are generated by calling the specified Python script's
# generate() function. The 'faker' library is available and suggested
# Each app thus generators items appropriate for its use case
./bulk fill --table t --generator fakeusers --numitems 100000000


# Add a "touched" timestamp attribute to all items
# The logic inside touched.py sees each item and returns the needed update call, if any
./bulk update --table t --generator touched


# Run a count using a direct scan
# This can be more efficient than using the DynamoDB connector by avoiding loading items into Glue memory
./bulk scancount --table t

# The scancount can take a common DynamoDB filter-expression syntax
# and runs it by pushing down the predicate for max speed
# You can specify expression names and values, just always
# use single quotes around the JSON and double quotes within!
./bulk scancount --table t --filter-expression "#touched > :touched" --expression-names '{"#touched": "touched"}' --expression-values '{":touched":1742359403.0}'

# The scancount can also operate against an index
# Count items present in a sparse GSI
./bulk scancount --table t --index i 

# You can add a filter expression
# Use single quotes around the JSON and double quotes within!
./bulk scancount --table t --index i --filter-expression "a = :aval" --expression-values '{":aval":"foo"}'


# Compare two tables for differences (uses segmented scans internally)
./bulk diff --table t --table2 t2

# The default diff format is "keys" to show primary keys having changes with +/-/* for adds/removes/changes
# Specifying the format "full" outputs the total items with +/- for before/after
# Output can be directed to to S3
./bulk diff --table t --table2 t2 --format full --s3

# For speed and cost reasons, you can diff a sample of both tables, here 10%
./bulk diff --table t --table2 t2 --sample-fraction 0.1


# Load from CSV held in S3 to existing tables
# Internally use the Glue connectors to CSV data sources
# https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-csv-home.html
./bulk load --table target --format csv --s3-path "s3://..."

# Load from JSON (line-oriented JSON, not DDB-JSON)
# https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-json-home.html
./bulk load --table target --format json --s3-path "s3://..."

# Load from Parquet
# https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-parquet-home.html
./bulk load --table target --format parquet --s3-path "s3://..."


# Run Spark SQL (similar to Athena), with an optional limit
# See https://spark.apache.org/docs/latest/api/sql/index.html
./bulk sql --table target --query "select ..." --limit 100
```

## Quick start

If you just want to test things out, here's the quick recipe. More detail follows.

1. Choose a local laptop or Amazon EC2 machine as your client. It should have `python` installed with access to proper AWS credentails and configuration.
1. Install boto3: `pip install -r requirements.txt`
1. Clone the repo
1. Bootstrap the environment (this makes a Glue job, role, and supporting S3 bucket)
    1. `./bulk bootstrap --XRole READ-WRITE`
    2. Saying READ-WRITE means you want the Glue job to have read and write privs. If you don’t do XRole you get an interactive prompt walking you through this.
1. Make an on-demand table called `mytable` with classic pk/sk schema and a higher warm throughput.

```
aws dynamodb create-table \
    --table-name mytable \
    --attribute-definitions AttributeName=pk,AttributeType=S AttributeName=sk,AttributeType=S \
    --key-schema AttributeName=pk,KeyType=HASH AttributeName=sk,KeyType=RANGE \
    --billing-mode PAY_PER_REQUEST \
    --warm-throughput WriteUnitsPerSecond=40000
```

6. Enable PITR on the table. Bulk Executor will NOT let you make mutations on a table unless PITR is enabled. You can’t do this call too quickly after the table create or you’ll get an error.

```

aws dynamodb update-continuous-backups \
--table-name mytable \
--point-in-time-recovery-specification PointInTimeRecoveryEnabled=true

```

7. Load a million sample lines of data.
    1. `./bulk fill --table mytable --generator default --numitems 1000000`
    2. This will take about 2 minutes. There’s about a minute of overhead for any Glue action. 
    3. You can do more than a million if you want. Perhaps you want a large table and small table both, for testing.
    4. This uses a simple default generator available with the `fill` script.
8. Do a live count to ensure you loaded successfully
    1. `./bulk scancount --table mytable`
    2. This will take about 2 minutes again.
9. Experiment with other commands, perhaps `find` as a good next step


## Installation and bootstrap

### Decide where to run the code

You can run this on your laptop or an EC2 environment. A laptop is no slower, because most of the activity happens within the AWS context.

You should NOT pick an environment where a VPC Endpoint for CloudWatch Logs has been setup, because the bulk tool relies on CloudWatch LiveTail which does not work in such an environment.

CloudShell is such an environment so be careful the bulk tool does not work readily from CloudShell. It will work in CloudShell only if you use a different region as your target, because that avoids the local VPC Endpoint.

### Install the dependencies

The only custom dependency is boto3, which can be installed like this if not already present:

`pip install -r requirements.txt`

### Setup authentication

The bulk tool relies on the standard Python boto3 SDK discovery mechanism to locate the authentication tokens and determine the default region. See https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html. 

You can use `~/.aws/credentials` and `~/.aws/config` or set environment variables like `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_SESSION_TOKEN`, and `AWS_DEFAULT_REGION`.

### Security

A word on permissions. We assume there exist two tiers of users:

* A powerful administrative user with a robust set of permissions around Glue, S3, and IAM. This user runs a command to bootstrap the Glue environment and create the supporting roles and resources.
* Any number of common users that use the Glue context in order to run the bulk tasks. These users don't need special permissions beyond the ability to invoke the Glue job, look at the resulting CloudWatch Logs, and read output from the associated S3 bucket.

### Bootstrap

The bootstrapping step:

* Creates an S3 bucket to hold the execution scripts (as well as persisting found records, enabling rate limit coordination among executors, etc). The bucket will follow the naming pattern `aws-glue-bulk-dynamodb-{aws_region}-{aws_account_id}-{random}`.
* Uploads the S3 scripts from the local folder to the right location within the S3 bucket. The Glue job will use these scripts for execution.
* Creates a role for the Glue execution, or attaches a pre-existing role (if specified).
* Creates the actual Glue job that will perform bulk actions.

The bootstrap must be performed by a role with this policy at minimum:

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "wildcards",
            "Effect": "Allow",
            "Action": [
                "iam:ListAttachedRolePolicies",
                "iam:GetRole",
                "iam:CreateRole",
                "iam:DeleteRole",
                "iam:AttachRolePolicy",
                "iam:DetachRolePolicy"
            ],
            "Resource": "*"
        },
        {
            "Sid": "passrole",
            "Effect": "Allow",
            "Action": [
                "iam:PassRole"
            ],
            "Resource": "arn:aws:iam::*:role/AWSGlueServiceRole*",
            "Condition": {
                "StringLike": {
                    "iam:PassedToService": [
                        "glue.amazonaws.com"
                    ]
                }
            }
        },        
        {
            "Sid": "s3",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:CreateBucket",
                "s3:ListBucket",
                "s3:DeleteBucket",
                "s3:PutBucketPolicy"
            ],
            "Resource": [
                "arn:aws:s3:::aws-glue-bulk-dynamodb-*"
            ]
        },
        {
            "Sid": "glue",
            "Effect": "Allow",
            "Action": [
                "glue:CreateJob",
                "glue:UpdateJob",
                "glue:DeleteJob",
                "glue:GetJob"
            ],
            "Resource": [
                "arn:aws:glue:*:*:job/bulk_dynamodb"
            ]
        },
        {
            "Sid": "logs",
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogGroup",
                "logs:PutRetentionPolicy"
            ],
            "Resource": [
                "arn:aws:logs:*:*:log-group:/aws-glue/jobs/*"
            ]
        }        
    ]
}
```

### Performing the bootstrap

The bootstrap action looks like a regular `bulk` command. This command creates the back-end Glue job and S3 bucket in the default or specified AWS Region, and creates a Role for Glue with access that you specify according to the `--XRole` parameter. You can specify magic values READ-ONLY or READ-WRITE to have the bootstrap create a role, or provide a custom pre-existing role name. 

```
# Bootstrap the Glue context, providing READ-ONLY access to DynamoDB tables
./bulk bootstrap --XRole READ-ONLY
```

If you want to allow mutations to tables, indicate you want to enable write access. This creates a Role with READ-WRITE access to DynamoDB tables:

```
# Bootstrap and have Glue use a role that has full DynamoDB access
./bulk bootstrap --XRole READ-WRITE
```

You can also exercise exact control of what permissions and policies the Glue role should have (reading from only certain tables, for example) by directly specifying a pre-made Role that you want Glue to use:

```
# Bootstrap and provide Glue with the specific role
./bulk bootstrap --XRole rolename
```

If you provide a custom IAM role for your AWS Glue job:

* Ensure the role name starts with `AWSGlueServiceRole`
* Ensure the role has a trust policy that allows the Glue service principal (`glue.amazonaws.com`) to assume the role.
* Attach the managed policy `AWSGlueServiceRole` to grant Glue its baseline execution permissions.
* Attach `ServiceQuotasReadOnlyAccess` to allow the job to read service quota information (used to detect account-level read/write limits), or for maximum lockdown allow the `pricing:GetProducts` action.
* Attach `AWSPriceListServiceFullAccess` to allow the job to query AWS pricing APIs (used to estimate DynamoDB operation costs), or for maximum lockdown allow the `servicequotas:GetServiceQuota` and `servicequotas:GetAWSDefaultServiceQuota` actions.
* Add custom IAM permissions for DynamoDB access. You may attach `AmazonDynamoDBReadOnlyAccess` or `AmazonDynamoDBFullAccess`, or define a more restrictive policy targeting specific tables.

### Security: Consider adjusting S3 bucket behaviors

The bootstrap step creates an S3 bucket with the name `aws-glue-bulk-dynamodb-{region}-{account_id}-{random}`. After creation, you may choose to adjust the bucket settings according to https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html.

For example, you may choose to:

* Change the encryption policy (to change the key management selection)
* Add lifecycle policies (to manage data retention)
* Implement bucket versioning (to track changes)

The bucket already enforces the use of TLS for transit.

### Security: Consider adjusting CloudWatch settings

The Glue execution will write logs to CloudWatch logs groups in the deployment account. The bootstrap step creates the log groups if they don't already exist and sets their retention to 365 days. If you prefer a different duration, you can adjust the `LogRetentionPeriod` parameter for each log group.



### Clean up using teardown

If you ever want to remove the Glue job created during the bootstrap (and any roles created during the bootstrap), you can do so with the teardown command:

```
./bulk teardown
```

The teardown process will not remove any output written to S3 during bulk execution jobs.

## Run the bulk actions

Once you've completed bootstrapping, the regular bulk script actions can be performed by anyone with permission to run the Glue job. There are various actions available out of the box: `count`, `find`, `delete`, `fill`, `update`, `scancount`, `diff`, and `sql`.

Executing a bulk job:

* Invokes the Glue job with a set of parameters defining the bulk job.
* Pulls the results from CloudWatch for display on the command line.
* Possibly coordinates executor rate limiting using the S3 bucket.
* Writes any large output to the S3 bucket.

The execution must be performed by a role having this policy at minimum:

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "glue",
            "Effect": "Allow",
            "Action": [
                "glue:StartJobRun",
                "glue:GetJobRun",
                "glue:GetJob",
                "glue:BatchStopJobRun"
            ],
            "Resource": [
                "arn:aws:glue:*:*:job/bulk_dynamodb"
            ]
        },
        {
            "Sid": "logs",
            "Effect": "Allow",
            "Action": [
                "logs:StartLiveTail",
                "logs:DescribeLogGroups",
                "logs:DescribeLogStreams",
                "logs:FilterLogEvents"
            ],
            "Resource": [
                "arn:aws:logs:*:*:log-group::log-stream:",
                "arn:aws:logs:*:*:log-group:/aws-glue/jobs/*",
                "arn:aws:logs:*:*:log-group:/aws-glue/jobs/*:log-stream:*"
            ]
        },
        {
            "Sid": "ddb",
            "Effect": "Allow",
            "Action": [
                "dynamodb:DescribeTable"
            ],
            "Resource": [
                "arn:aws:dynamodb:*:*:table/*"
            ]
        }
    ]
}
```

If you ever want to stop execution early, you can hit Control-C. The interrupt will be caught and will cancel the background Glue job then cancel the command line program.

#### `count`

* Performs a parallel scan to find matching items, using the Glue DynamoDB Connector.
* Requires a `table` parameter
* Optional `where` parameter that specifies a match criteria for the item, using Spark SQL syntax. 
    * See https://spark.apache.org/docs/latest/api/sql/index.html
    * Note that whitespace around operators is required.
        * `--where "pk = '92zk'"`
        * `--where "payload > 'a'"`
        * `--where "payload between 'a' and 'z'"`
        * `--where "payload LIKE 'foo%'"`
* Optional `orderby` parameter that specifies the attribute(s) to sort by. If specifying multiple, use a comma separated list. Each attribute can include an `asc` and `desc` modifier. Default is `asc`. If the set of items to be ordered is large, you may need to increase the worker type. See below.
* Optional `limit` parameter that indicates counting can stop once this limit is reached.

#### `find`

* Performs a parallel scan to find and print matching items. Up to 10 items are printed to the console. Large output is written to S3 at a location specified in the output.
* Same parameters as `count`.

#### `delete`

* Performs a parallel scan to find and then delete matching items. 
* Same parameters as `count` and `find`.
* Remember: the `bootstrap` must have given Glue rights to modify any table.
* All mutation commands require Point in Time Recovery (PITR) enabled on the table for safety.

#### `fill`

* Performs a parallel insert of items, based on the items returned by a specified Python generator script. The Python script can generate realistic looking (but fake) data. It should return either one item or a list of items to be loaded. Returning a list can be useful when generating realistic-looking whole item collections with different entity types.
* Requires a `generator` parameter to point at the Python script to use. The Glue job calls the `generate()` function within that script repeatedly and in parallel to generate the items to use for filling the table. The script must exist in the S3 bucket as prepared during the bootstrap. In the likely event you want to use your own generator script, make sure the entity with bootstrap permissions runs bootstrap and includes your generator. The generator scripts are found under the `fill` folder. The generator can return a single item or a list of items. Some sample scripts are provided
* Required `numitems` parameter to indicate how many items to load.
* We suggest you consider the [Faker library](https://faker.readthedocs.io/en/master/) for generating the fake data. The Glue job includes the Faker library for your convenience.

#### `update`

* Performs a parallel update of items, based on the update expression keys returned by a Python script. The Python script gets handed each item in the table and can return empty if no update is needed or the parameters for an update expression if an update is needed. The generator script is not expected to perform the update itself.
* Requires a `generator` parameter to point at the Python script to use. The Glue job calls the `generate()` function within that script repeatedly and in parallel to generate the update expressions to use. The script must exist in the S3 bucket as prepared during the bootstrap. In the likely event you want to use your own generator script, make sure the entity with bootstrap permissions runs bootstrap and includes your generator. The generator scripts are found under the `update` folder.
* This script does not take a `where` parameter. That logic goes in the generator.

#### `scancount`

* Performs a parallel scan to count items. Leverages DynamoDB's `Select=COUNT` parameter on the `scan` call so only a count is returned on each internal DynamoDB scan call for maximum performance and memory efficiency.
* Accepts an optional `index` name to scan an index, suitable if there's an appropriate sparse index that would be faster to scan than the base table.
* Accepts a `filter-expression` to filter down the items counted. This expression uses the usual DynamoDB syntax. Requires a supporting  `expression-values` parameter and sometimes `expression-names`, as with usual DynamoDB scan calls.

#### `diff`

* Performs parallel scans to two tables to compare for differences.
* Requires `table` and `table2` parameters.
* Accepts an optional `format` which can be `compact` (prints primary keys of all changed items with `+`, `-`, `*` for adds, removes, changes) or `full` (prints the full values of all changed items with `+` and `-` for before and after). Default is `compact`.
* Accepts an optional `sample-fraction` to indicate a fraction of the two tables to compare, between 0.0 and 1.0. Can help with sanity checking that runs faster and at lower cost.
* Accepts an optional `s3` parameter to output the result to S3.

#### `load`

* Loads content from a CSV, JSON, or Parquet file.
* Requires a `table` parameter to load into
* Requires a `format` parameter specifying `csv`, `json`, or `parquet`
* Requires an `s3-path` parameter specifying the source location
* Accepts a variety of optional parameters controlling how to process the file 
* All formats accept:
    * `removeEmptyStringAttributes`: Indicates that attributes with empty strings can be left out of the resulting items, useful especially for CSV files with sparse data 
* CSV
    * `separator`: Specifies the delimiter character. The default is a comma, but any other character can be specified.
    * `escaper`: Specifies a character to use for escaping. If enabled, the character that immediately follows is used as-is, except for a small set of well-known escapes (`\n`, `\r`, `\t`, and `\0`). Default: none.
    * `quoteChar`: Specifies the character to use for quoting. The default is a double quote. Set this to `-1` to turn off quoting entirely.
    * `multiLine`: Specifies that a single record can span multiple lines. This can occur when a field contains a quoted new-line character. You must set this option if any record spans multiple lines. Enabling `multiLine` might decrease performance because it requires more cautious file-splitting while parsing.
    * `withHeader`: Specifies explicitly to treat the first line as a header, to indicate attribute names. Default true.
    * `withoutHeader`: Specifies explicitly to not treat the first line as a header. When specified the `mappings` parameter also needs to be specified.
    * `mappings`: Amazon S3 path in format like s3://aws-glue-target/mappings.json that points to a file describing the mappings between the auto generated column names (col0, col1, col2, etc) and also allows to specify a different type than String. Note that these Glue attribute types, not DynamoDB attribute types. For example, `int` instead of `Number`. You'll find an example mapping file and test data in the same folder as the source.
    * `skipFirst`: Indicates to skip the first data line. Default not. Useful if the first line is something like a comment.
* JSON
    * `jsonPath`: A [JsonPath expression](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-json-home.html) that identifies an object to be read into records. This is particularly useful when a file contains records nested inside an outer array. For example, the following JsonPath expression targets the id field of a JSON object: `format="json", format_options={"jsonPath": "$.id"}`
    * `multiLine`: Specifies that a single record can span multiple lines. This can occur when a field contains a quoted new-line character. You must set this option if any record spans multiple lines. Enabling `multiLine` might decrease performance because it requires more cautious file-splitting while parsing.
* Parquet
    * `compression`: Specifies the compression codec used. Default: `snappy`. Possible values: `uncompressed`, `snappy`, `gzip`, and `lzo`.
    * `blockSize`: Specifies the size in bytes of a row group being buffered in memory. You use this for tuning performance. Size should divide exactly into a number of megabytes. Default: `134217728` (equal to 128 MB).
    * `pageSize`: Specifies the size in bytes of a page. You use this for tuning performance. A page is the smallest unit that must be read fully to access a single record. Default: `1048576` (equal to 1 MB).

**`sql`**

* Performs a scan-driven execution of arbitrary Spark SQL against a table, useful for aggregates.
* Requires `table` and `query` parameters
* Optional `limit` parameter to limit the number of output items
* Only `SELECT` queries are supported

## Glue execution parameters

A few parameters, each starting with `X` to differentiate them from regular script parameters, can be used to influence the Glue execution behavior:

* `XExecutionClass`: Can be set to `STANDARD` (default) or `FLEX`. Flex jobs run on spare capacity and have lower DPU pricing. See https://aws.amazon.com/about-aws/whats-new/2022/08/aws-glue-supports-flex-execution-option/.
    * However, testing a 1.7 billion item count job shows FLEX takes longer and burns more DPUs.
    * STANDARD run: `Job duration: 0:05:31.295028 (3.93 DPU hours)`
    * FLEX run: `Job duration: 0:11:34.849592 (7.57 DPU hours)`
* `XNumberOfWorkers`: Controls how many Glue workers to use, default 220. Small tables or jobs could set this lower.
* `XWorkerType`: Controls the worker type. Can be `G.1X` (default), `G.2X`, `G.4X`, `G.8X`, `G.12X`, `G.16X`, `R.1X`, `R.2X`, `R.4X`, or `R.8X`. Using a larger type provides more compute, memory, and storage for jobs that require a single worker to gather data locally, for example an `orderby` applied to a large set of matching items.
* `XRetries`: Controls the max number of Glue Job retries. Defaults to zero to fail a misconfigured Glue Job quickly. It's unlikely you'll need to need to change this.
* `XTimeout`: Controls the Glue Job timeout (in minutes). Default is 60 minutes.
* `XWaitForDPU`: If present, causes the execution to wait 40 seconds at the end of execution in order for the DPU metrics to arrive so they can be reported. Default is off.

Here is a sample `delete` action that adjusts some default Glue execution parameters:

```
# Run a large delete job using the FLEX type, 
# just 100 workers, but larger workers to enable the large sort
./bulk delete --table t --orderby timestamp --limit 10000000 \
       --XExecutionClass FLEX \
       --XNumberOfWorkers 100 --XWorkerType G.4X \
       --XWaitForDPU
```

## Throughput management

Use `XMaxReadRate` and `XMaxWriteRate` to indicate the maximum read or write units per second you want consumed during the operation against a table. If unspecified, the table is examined and the table's current abilities are used to determine the max rate. Note it's a maximum limit, not a target that can be reliably achieved.

* `--XMaxReadRate 40000`
* `--XMaxWriteRate 40000`

If you set a very low maximum read rate (below say 200) the effective maximum may still be around 200, because within Glue there are often many shards and each needs its own minimum allowance. Also it may take a few minutes of oscillation before the maximum becomes the steady-state rate.

These are provided as `--X` flags even though they're actually implemented inside the scripts, using libraries provided by the harness.

With `diff` which reads from two tables, the max read rate is applied per table.

## Cost management

At the start of each run, each command outputs discovered metrics about the table(s) being used and a cost estimate for any reads and writes that will be performed. If the estimate is too high, you can hit Control-C to cancel the execution. The estimates are rough and not guaranteed. Table metrics use the table metadata available when describing a table, which updates about every 4 hours.

At the beginning of each output you'll find this cost-related notice:
> The bulk executor job cost consists of DynamoDB and Glue costs  
For small jobs, the Glue cost portion is usually dominating  
Using fewer Glue workers for small jobs, through the --XNumberOfWorkers parameter, will often reduce the Glue costs  
For large jobs, where the cost is more significant, the DynamoDB cost portion is usually dominating  
The DynamoDB cost will be estimated below  
The Glue cost estimation isn't provided since it is based on DPU hours being used by the job, which is hard to estimate in advance  
You can run the script with the --XWaitForDPU parameter in order to print the used Glue DPU hours at the end of the job  

Note that commands that use the DynamoDB connector (`count`, `find`, `delete`, `sql`) will sometimes perform two scans when running against large tables. The cost estimate takes this into account.

## Extensibility (writing custom command scripts)

There are two parts to the bulk executor: the harness and the individual command scripts (the verbs). The core commands are each just Python scripts called by the harness and leveraging the parallelization abilities of Glue Spark. There's a client-side component to each command (a chance to do quick parameter validation, check for table existence, etc) and a server-side component (to perform the bulk execution).

A new command can be added by placing the new command scripts into the right client-side and server-side folders, doing a bootstrap, and running a bulk command requesting that command. The Glue harness handles the rest.

You're encouraged to write your own scripts if you have needs beyond those supported by the standard scripts. To write custom scripts, learn from and follow the code design shown by the standard command scripts. Note that the interface between the harness and the command scripts is not yet formalized. We expect the harness and scripts called by the harness to be co-developed and released together for the time being. If your custom script is generally useful, please consider contributing it to the repo, and then the script can be kept current with changes in the future. Otherwise assume that a custom script may be tied to the particular version of the bulk tool it was written against.

If writing a custom script, it's important to do proper exception handling.

* When in the Glue driver code block (main control flow), if the problem is a developer-type problem indicating a code issue, you can just let the underlying exception propagate and kill execution. Glue handles it nicely.
* When in the Glue driver code block, if the problem is instead a user-input type problem, you can notice the issue and raise it as `raise Exception("reason") from None`. This will halt execution and the `from None` (useful when inside an `except` clause) will suppress the original exception. We don't need bit stack traces for user-input type problems.
* When in a Glue executor code block (the code being run in parallel), do NOT raise an exception. If you do, usually lots of workers hit the same issue and it generates a fireworks of error output. Instead, use the `error_accumulator` pattern that the `fill` verb (and others) demonstrate. Add a description of the problem to the `error_accumulator` and return. In the driver code, find the first accumulated error and raise it.

